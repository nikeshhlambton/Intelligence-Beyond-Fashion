# -*- coding: utf-8 -*-
"""Intelligence Beyond Fashion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19mPtw5k6thYNM8IM02LQlRfPp5jbALKu

# **Team members**

1.   Nikeshh Vijayabaskaran - C0849544
2.   Raghavendra Reddy - C0851724
3.   Shravan Kumar Reddy - C0833124
4.   Jebin George - C0850509
5.   Deeksha Naikap - C0835440

# **GPU Setup and RAPIDS AI**
"""

!nvidia-smi

# Sample code available online to check running using gpu
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
with tf.device('/gpu:0'):
    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')
    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')
    c = tf.matmul(a, b)
with tf.compat.v1.Session() as sess:
    print (sess.run(c))

pip install pyarrow pynvml

!git clone https://github.com/rapidsai/rapidsai-csp-utils.git
!python rapidsai-csp-utils/colab/env-check.py

!bash rapidsai-csp-utils/colab/update_gcc.sh
import os
os._exit(00)

import condacolab
condacolab.install()

!python rapidsai-csp-utils/colab/install_rapids.py stable
import os

# Sample code available online to check the cudf functionality
import cudf
import io, requests

url="https://github.com/plotly/datasets/raw/master/tips.csv"
content = requests.get(url).content.decode('utf-8')

tips_df = cudf.read_csv(io.StringIO(content))
tips_df['tip_percentage'] = tips_df['tip']/tips_df['total_bill']*100

print(tips_df.groupby('size').tip_percentage.mean())

# Sample code available online to check the cudf functionality
import cuml

df_float = cudf.DataFrame()
df_float['0'] = [1.0, 2.0, 5.0]
df_float['1'] = [4.0, 2.0, 1.0]
df_float['2'] = [4.0, 2.0, 1.0]

dbscan_float = cuml.DBSCAN(eps=1.0, min_samples=1)
dbscan_float.fit(df_float)

print(dbscan_float.labels_)

"""# **Preparation**

## **Mount Google Drive and Install the dependencies**
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install sentence_transformers

"""## **Loading Dependencies**"""

# General
import os
os.chdir('/content/')
from tqdm.notebook import tqdm
import pickle
import gc, math
import csv
import sys
import joblib
sys.modules['sklearn.externals.joblib'] = joblib
from pathlib import Path
from random import randrange
# Data manipulation
from pylab import *
import pandas as pd
import numpy as np
# Text Processing
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
# Image Processing
import cv2
import imageio as io
from IPython.display import Image
from PIL import Image
# Text and Image Analysis
from sklearn.metrics.pairwise import cosine_similarity
# Tensorflow, Sklearn and Keras Libraries
import tensorflow as tf
import keras
from keras.applications.vgg16 import preprocess_input
from sklearn.cluster import KMeans
from keras import backend as K
from keras.models import load_model
from sklearn.neighbors import KNeighborsClassifier
import keras.utils as image
from sklearn.preprocessing import LabelEncoder
from sklearn.manifold import TSNE
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, MaxPool2D
from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import load_img
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.applications import EfficientNetB5 # Pretrained model - Used only for comparison
# Visualization
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import plotly.graph_objects as go

"""## **Importing Data and Exploration**"""

# Unzip data
!unzip "/content/drive/My Drive/Intelligence beyond fashion/data.zip"

# Get all the labels
labels = sorted(os.listdir('data/'))
main_labels = labels
print(labels)

path = Path('data')
def displayRandomImagesFromEveryFolder(directory=path, samplesPerFolder=5):
    fig = plt.figure(figsize=(20,15))
    for rowIndex in range(1, len(labels)):
        subdirectory = str(rowIndex)
        path = directory/labels[rowIndex - 1]
        images = sorted(os.listdir(path))
        for sampleIndex in range(1, samplesPerFolder+1):
            randomNumber = randrange(len(images)-1)
            image = Image.open(path/images[randomNumber])
            ax = fig.add_subplot(45, 5, (samplesPerFolder*rowIndex + sampleIndex))
            ax.axis("off")
            plt.title(str(labels[rowIndex -1]))
            plt.imshow(image, cmap='gray')
    plt.show()

# Displaying images and check if everything is correct
displayRandomImagesFromEveryFolder()

# Finding the number of images in each label
number_of_data_in_each_class = []
for label in labels:
    path = 'data/{0}/'.format(label)
    folder_data = sorted(os.listdir(path))
    count = 0
    for image_path in folder_data:
        if count < 5:
            pass
        count = count+1
    number_of_data_in_each_class.append(count)

# Plotting the variation of the number of images in each label across a interactive bar chart
fig = go.Figure(data=[go.Bar(
            x=labels, y=number_of_data_in_each_class,
            text=number_of_data_in_each_class,
            textposition='auto',
        )])
fig.update_layout(title_text='NUMBER OF IMAGES CONTAINED IN EACH CLASS')
fig.show()

# Printing all the images in each class to see if there is any wrong files
for label in labels:
    path = 'data/{0}/'.format(label)
    folder_data = sorted(os.listdir(path))
    print(folder_data)

# Convert the label and image data into numpy array
# x_data points to the image data
# y_data points to the label
x_data = []
y_data = []
for label in labels:
    path = 'data/{0}/'.format(label)
    folder_data = sorted(os.listdir(path))
    for image_path in folder_data:
        if image_path:
            image = cv2.imread(path+image_path)
            image_resized = cv2.resize(image, (120,120))
            x_data.append(np.array(image_resized))
            y_data.append(label)
        pass
    pass

x_data = np.array(x_data)
y_data = np.array(y_data)
print('the shape of X is: ', x_data.shape, 'and that of Y is: ', y_data.shape)

x_data = x_data.astype('float32') / 255

# Get the categorical data based on the label encoder
y_categorical = to_categorical(LabelEncoder().fit_transform(y_data))

# Obtain the train, test and validation data
arranged_data = np.arange(x_data.shape[0])
X = x_data[arranged_data]
Y = y_categorical[arranged_data]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)

X_train, X_val , Y_train , y_val = train_test_split(X_train, Y_train, test_size = 0.1 , random_state = 2020)

X_train.shape, Y_train.shape

X_val.shape, y_val.shape

# Extract the product details from the csv files
!unzip "/content/drive/My Drive/Intelligence beyond fashion/metadata.zip"

# Function to check if the image exists in a path
def image_exists(path):
  return os.path.exists(path)

# Construct a single csv based on the multiple csv files obtained
directory = 'metadata'
with open('input.csv', 'w', newline='') as input:
  writer = csv.writer(input)
  columns = ["Label", "OriginalName", "ProductName", "Title", "Price"]
  writer.writerow(columns)
  for filename in sorted(os.listdir(directory)):
    f = os.path.join(directory, filename)
    if os.path.isfile(f) and filename.endswith('.csv'):
      with open(f, encoding = 'cp850') as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        for column in csv_reader:
            label = filename.split(".")[0]
            column1 = column[1]
            column2 = column[2]
            column3 = column[3]
            if(column1 != "Image" and column2 != "Title" and column3 != "Price" and column1 != "" and column2 != "" and column3 != "" and image_exists("./data/" + label + "/" + column1)):
              writer.writerow([label, column1, label + "_" + column1, column2, column3])

# Load the input csv and save it to the google drive
input = pd.read_csv("./input.csv", sep=",")
input.to_csv('/content/drive/My Drive/Intelligence beyond fashion/input.csv')
input.columns

input

# Form the image path and label
input['image_path'] = "./data/" + input['Label'] + "/" + input['OriginalName']
input['label_number'] = np.random.randint(0,100)

input

"""
Convert given set of images to array
@param - images_path - (list) - list of images_path
@param - shape - (int) - shape of the array
@return - ndarray contains a array of images
"""
def convert_images_to_array(images_path, shape):
    image_array = []
    for path in tqdm(images_path):
        img = cv2.imread(path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = cv2.resize(img, (shape, shape))
        image_array.append(np.array(img))
    image_array = np.array(image_array)
    image_array = image_array.reshape(image_array.shape[0], shape, shape, 3) 
    image_array = image_array.astype('float32')
    image_array /= 255 
    return np.array(image_array)

# Split the image path into train and test files (Used for image similarity matching)
train_files, test_files = train_test_split(input['image_path'], test_size = 0.002)

train_data = convert_images_to_array(train_files, 224)
test_data = convert_images_to_array(test_files, 224)
print("Traning dataset length:", train_data.shape)
print("Test dataset length:", test_data.shape)

"""## **Plotting**"""

# Function to plot the images based on the parameters specified
def plot_custom(x_values, y_values_1, y_values_2='', num_rows=1, num_cols=1, plot_index=1, plot_title='', x_label='', y_label='', plot_label='', is_image=False, line_color='b'):
    plt.subplot(num_rows, num_cols, plot_index)
    
    if is_image:
        plt.imshow(x_values)
        plt.title(plot_title)
        plt.axis('off')
    else:
        plt.plot(y_values_1, label=plot_label, color='g')
        plt.scatter(x_values, y_values_1, color='g')
        
        if y_values_2 != '':
            plt.plot(y_values_2, color=line_color, label='validation')
            plt.scatter(x_values, y_values_2, color=line_color)
        
        plt.grid()
        plt.legend()
        plt.title(plot_title)
        plt.xlabel(x_label)
        plt.ylabel(y_label)

"""# **Text Model and Prediction**"""

test_value = "Travel Gym Duffel Bag"

"""## Sentence Transformer"""

# Initialize the sentence transformer and get the embeddings based on the title values
text_model = SentenceTransformer('stsb-mpnet-base-v2')
text_embeddings = text_model.encode(input.Title.values)
pickle.dump(text_model, open('/content/drive/My Drive/Intelligence beyond fashion/sentence_transformers_text_model.sav', 'wb'))
np.save('/content/drive/My Drive/Intelligence beyond fashion/sentence_transformer_text_embeddings.npy', text_embeddings)
print(text_embeddings.shape)

# Normalize Embeddings
def normalize_embeddings(embeddings):
    for embedding in embeddings:
        temp = np.linalg.norm(embedding)
        embedding/=temp
    return embeddings

# Function to predict based on the cosine value
def cosine_value_predictions(embeddings, curr_embedding):
    cosine_value = np.matmul(embeddings, curr_embedding.T).T
    reshape_data = np.reshape(cosine_value > 0.0,(len(embeddings),))
    input['values'] = np.reshape(cosine_value,(len(embeddings),))
    return input[reshape_data].sort_values(by='values', ascending=False)

# Getting the saved text model embeddings
sentence_transformer_text_embeddings = normalize_embeddings(np.load("/content/drive/My Drive/Intelligence beyond fashion/sentence_transformer_text_embeddings.npy"))

# Encode the test value and obtain the embedding
input_text_embedding = normalize_embeddings(text_model.encode(test_value))

# Get the text predictions based on the cosine similarity
text_predictions = cosine_value_predictions(sentence_transformer_text_embeddings, input_text_embedding)

# Get the top 15 predictions
text_predictions_top_15 = text_predictions.iloc[:15].reset_index()
text_predictions_top_15

related_top_15_text_data = text_predictions_top_15.Title.values
related_top_15_text_data = [text.strip() for text in related_top_15_text_data]
related_top_15_text_data

"""## TFIDF Vectorizer"""

# Initalize TFIDF Vectorizer and generate a sparse matrix. Based on the sparse matrix, generate the cosine similarity matrix
tfidfVectorizer = TfidfVectorizer()
sparse_matrix = tfidfVectorizer.fit_transform([test_value] + input.Title)
cosine_similarity_matrix = cosine_similarity(sparse_matrix[0,:], sparse_matrix[1:,:])

# Get the text predictions based on the cosine similarity
text_predictions = pd.DataFrame({'Title': input.Title[1:], 'values': cosine_similarity_matrix[0]})
text_predictions = text_predictions.sort_values('values', ascending=False)

# Get the top 15 text predictions
text_predictions_top_15 = text_predictions.iloc[:15].reset_index()
text_predictions_top_15

related_top_15_text_data = text_predictions_top_15.Title.values
related_top_15_text_data = [text.strip() for text in related_top_15_text_data]
related_top_15_text_data

"""# **Image Model and Prediction**

## Own Models

#### Data preparation

Below low resolution image data is used only for autoencoder decoder type of models. This is because this type of models require to encode and decode on a low resolution image data
"""

train_x = train_data
val_x = test_data

# Inorder for analysis, it is important to reduce the image resolution.
def reduce_image_resolution(img, scale_pct = 40):
  width = int(img.shape[1] * scale_pct / 100)
  height = int(img.shape[0] * scale_pct / 100)
  dim = (width, height)
  small_img = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)
  
  # Scaling back to original size
  new_width = 224
  new_height = 224
  new_dim = (new_width, new_height)

  low_res_img = cv2.resize(small_img, new_dim, interpolation=cv2.INTER_AREA)

  return low_res_img

# Training set - Generate low resolution images
train_x_px = []
for i in range(train_x.shape[0]):
  temp = reduce_image_resolution(train_x[i,:,:,:])
  train_x_px.append(temp)
train_x_px = np.array(train_x_px)

# Validation set - Generate low resolution images
val_x_px = []
for i in range(val_x.shape[0]):
  temp = reduce_image_resolution(val_x[i,:,:,:])
  val_x_px.append(temp)
val_x_px = np.array(val_x_px)

"""#### Model 1 - Autoencoder decoder model

"""

def autoencoder_decoder_first_image_model():
  input = Input(shape=(224, 224, 3))
  # Encoding Architecture
  x1 = Conv2D(224, (3, 3), activation='relu', padding='same')(input)
  x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x1)
  x2 = MaxPool2D( (2, 2))(x2)
  encoded = Conv2D(64, (3, 3), activation='relu', padding='same')(x2)

  # Decoding Architecture
  x3 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)
  x3 = UpSampling2D((2, 2))(x3)
  x2 = Conv2D(128, (3, 3), activation='relu', padding='same')(x3)
  x1 = Conv2D(224, (3, 3), activation='relu', padding='same')(x2)
  decoded = Conv2D(3, (3, 3), padding='same')(x1)
  autoencoder = Model(input, decoded)
  autoencoder.compile(optimizer='adam', loss ='mse', metrics = ['accuracy'])
  return autoencoder

autoencoder_decoder_first_image_model = autoencoder_decoder_first_image_model()
early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='auto')
a_e = autoencoder_decoder_first_image_model.fit(train_x_px, train_x,
            epochs=100,
            batch_size=8,
            shuffle=True,
            validation_data=(val_x_px, val_x),
            callbacks=[early_stopper])

sample_image = train_data[7]
sample_image = np.expand_dims(sample_image,axis=0)
image = autoencoder_decoder_first_image_model.predict(sample_image)
plot_custom(sample_image[0,:,:,:],'','',1,2,1,"Orginal Image","","","",True)
plot_custom(image[0,:,:],'','',1,2,2,"Decoded Image","","","",True)
plt.show()

autoencoder_decoder_first_image_model.summary()
print("\n")
tf.keras.utils.plot_model(autoencoder_decoder_first_image_model, to_file='/content/drive/My Drive/Intelligence beyond fashion/autoencoder_decoder_first_image_model.png')

autoencoder_decoder_first_image_model.save('/content/drive/My Drive/Intelligence beyond fashion/autoencoder_decoder_first_image_model.h5')
#autoencoder_decoder_first_image_model.save_weights('/content/drive/My Drive/Intelligence beyond fashion/autoencoder_decoder_first_image_model_weights.h5')

"""#### Model 2 - Autoencoder decoder model"""

def autoencoder_decoder_second_image_model():
    model = Sequential(name='Convolutional_AutoEncoder_Decoder_Second_Image_Model')
    # Encoding Architecture
    model.add(Conv2D(224, kernel_size=(3, 3),activation='relu',input_shape=(224, 224, 3),padding='same', name='Encoding_Conv2D_1'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', name='Encoding_MaxPooling2D_1'))
    model.add(Conv2D(128, kernel_size=(3, 3),strides=1,kernel_regularizer = tf.keras.regularizers.L2(0.001),activation='relu',padding='same', name='Encoding_Conv2D_2'))
    model.add(MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', name='Encoding_MaxPooling2D_2'))
    model.add(Conv2D(64, kernel_size=(3, 3),activation='relu',input_shape=(224, 224, 3),padding='same', name='Encoding_Conv2D_3'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=2, padding='valid'))

    # Decoding Architecture
    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_regularizer = tf.keras.regularizers.L2(0.001), padding='same',name='Decoding_Conv2D_1'))
    model.add(UpSampling2D((2, 2),name='Decoding_Upsamping2D_1'))
    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_regularizer = tf.keras.regularizers.L2(0.001), padding='same',name='Decoding_Conv2D_2'))
    model.add(UpSampling2D((2, 2),name='Decoding_Upsamping2D_2'))
    model.add(Conv2D(224, kernel_size=(3, 3), kernel_regularizer = tf.keras.regularizers.L2(0.001), activation='relu', padding='same',name='Decoding_Conv2D_3'))
    model.add(UpSampling2D((2, 2),name='Decoding_Upsamping2D_3'))
    model.add(Conv2D(3, kernel_size=(3, 3), padding='same', activation='sigmoid', name='Decoding_Output'))
    
    return model

model = autoencoder_decoder_second_image_model()
model.summary()
print("\n")
tf.keras.utils.plot_model(model, to_file='/content/drive/My Drive/Intelligence beyond fashion/autoencoder_decoder_second_image_model.png')

early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='auto')

"""Due to the computational power - maximum ram usage problem, hyperparameter tuning is processed in batches for each optimizer type (Adagrad, Adam, Rmsprop). Also, epoch of 20 is used to compare the results"""

# Hyperparameter tuning
# parameters = {'Adagrad':[0.01,0.001,0.0001,0.00001]}
parameters = {'Adam':[0.0001,0.00001]}
# parameters = {'Rmsprop':[0.01,0.001,0.0001,0.00001]}
results = []
for i in parameters.keys():
    print("Optimizer: " + i)
    values = parameters[i]
    result = []
    for learning_rate in values:
        print("\t\tUsing learning_rate: "+ str(learning_rate))
        print("Learning Rate: " + str(learning_rate))
        model = autoencoder_decoder_second_image_model()
        if i=='Adam':
            optimizer = Adam(learning_rate=learning_rate)
        elif i=='Adagrad':
            optimizer = Adagrad(learning_rate=learning_rate)
        else:
            optimizer = RMSprop(learning_rate=learning_rate)
        model.compile(optimizer=optimizer, loss ='mse', metrics = ['accuracy'])
        model.fit(train_x_px, train_x,
                    epochs=20,
                    batch_size=8,
                    shuffle=True,
                    validation_data=(val_x_px, val_x),
                    callbacks=[early_stopper])
        result.append(model.history.history)
    results.append(result)

"""All the optimizers and learning rates are run in batches by changing the parameters array in above code which gave the following results.

Random run result:

**Adagrad:**
1. 0.01: 60.41%
2. 0.001: 61.24%
3. 0.0001: 59.21%
4. 0.00001: 61.10%,

**Adam:**
1. 0.01: 71.55%
2. 0.001: 60.86%
3. 0.0001: 69.64%
4. 0.00001: 61.12%
  
**Rmsprop:**
1. 0.01: 62%
2. 0.001: 61.57%
3. 0.0001: 61.16%
4. 0.00001: 62.52%

On multiple runs, Adam with learning rate 0.0001 gave the best results. However, the results seem to vary for each run.
"""

optimizer = Adam(learning_rate=0.0001) 
model = autoencoder_decoder_second_image_model()
model.compile(optimizer=optimizer, loss ='mse', metrics = ['accuracy']) 
checkpoint = ModelCheckpoint('/content/drive/My Drive/Intelligence beyond fashion/autoencoder_decoder_second_image_model.h5', monitor='val_loss', mode='min', save_best_only=True)

model.fit(train_x_px, train_x,
            epochs=100,
            batch_size=8,
            shuffle=True,
            validation_data=(val_x_px, val_x),
            callbacks=[early_stopper, checkpoint])

# Analyzing the training and validation loss
loss = model.history.history["loss"]
val_loss = model.history.history["val_loss"]
plt.figure(figsize=(15,5))
epochs = [i for i in range(len(loss))]
plot_custom(epochs,loss,'',1,2,1,'Training loss on each epoch','Epoch','Loss','training',False,'g')
epochs = [i for i in range(len(val_loss))]
plot_custom(epochs,val_loss,'',1,2,2,'validation loss on each epoch','Epoch','Loss','testing',False,'r')

# Loading the saved model
model = load_model("/content/drive/My Drive/Intelligence beyond fashion/autoencoder_decoder_second_image_model.h5")
model.compile(optimizer=optimizer, loss='mse', metrics = ['accuracy'])

# Testing the prediction of the model
sample_image = train_data[7]
sample_image = np.expand_dims(sample_image,axis=0)
image = model.predict(sample_image)
plot_custom(sample_image[0,:,:,:],'','',1,2,1,"Orginal Image","","","",True)
plot_custom(image[0,:,:],'','',1,2,2,"Decoded Image","","","",True)
plt.show()

sample_image = train_data[2396]
sample_image = np.expand_dims(sample_image,axis=0)
image = model.predict(sample_image)
plot_custom(sample_image[0,:,:,:],'','',1,2,1,"Orginal Image","","","",True)
plot_custom(image[0,:,:],'','',1,2,2,"Decoded Image","","","",True)
plt.show()

"""#### Model 3 - CNN model"""

cnn_image_model = keras.models.Sequential([
      keras.layers.Conv2D(filters = 64 , kernel_size = 3,strides = (1,1), padding = 'valid',activation = 'relu',input_shape = [120,120,3]), # 1st Layer
      keras.layers.MaxPooling2D(pool_size = (2,2)),
      keras.layers.Conv2D(filters = 128 , kernel_size = 3,strides = (2,2), padding = 'same',activation = 'relu'), # 2nd Layer
      keras.layers.MaxPooling2D(pool_size = (2,2)),
      keras.layers.Conv2D(filters = 64 , kernel_size = 3,strides = (2,2), padding = 'same',activation = 'relu'), # 3rd Layer
      keras.layers.MaxPooling2D(pool_size = (2,2)),
      keras.layers.Flatten(),
      keras.layers.Dense(units = 128,activation = 'relu'),
      keras.layers.Dropout(0.25),
      keras.layers.Dense(units = 256,activation = 'relu'),
      keras.layers.Dropout(0.5),
      keras.layers.Dense(units = 256,activation = 'relu'),
      keras.layers.Dropout(0.25),
      keras.layers.Dense(units = 128,activation = 'relu'),
      keras.layers.Dropout(0.10),
      keras.layers.Dense(units = len(labels),activation = 'softmax')                   
])

cnn_image_model.summary()
print("\n")
tf.keras.utils.plot_model(cnn_image_model, to_file='/content/drive/My Drive/Intelligence beyond fashion/cnn_image_model.png')

cnn_image_model.compile(optimizer = 'adam', loss ='mse', metrics = ['accuracy'])

early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=10, verbose=1, mode='auto')

cnn_image_model.fit(X_train, Y_train, epochs=100, batch_size=8, shuffle=True, verbose = 1, validation_data=(X_val,y_val), callbacks=[early_stopper])

# Here, np.expand is used to change the 3 dimension data into 4 dimension
cnn_image_model.predict(np.expand_dims(X_test[0],axis = 0)).round(2)

np.argmax(cnn_image_model.predict(np.expand_dims(X_test[0],axis = 0)).round(2))

y_pred = cnn_image_model.predict(X_test).round(2)
y_pred

cnn_image_model.evaluate(X_test, Y_test)

label = np.array(labels)

plt.figure(figsize=(20,35))
j = 1
for i in np.random.randint(0,1000,48):
  plt.subplot(12,4,j); j+=1
  plt.imshow(cv2.cvtColor(X_test[i], cv2.COLOR_BGR2RGB))
  plt.axis('off')
  plt.title('Actual = {} / {} \nPredicted = {} / {}'.format(labels[np.argmax(Y_test[i])],np.argmax(Y_test[i]),labels[np.argmax(y_pred[i])],np.argmax(y_pred[i])))

cnn_image_model.save('/content/drive/My Drive/Intelligence beyond fashion/cnn_image_model.h5')

"""### Choosing the model

autoencoder_decoder_second_image_model seemed to perform better
"""

choosed_model = "/content/drive/My Drive/Intelligence beyond fashion/autoencoder_decoder_second_image_model.h5"
model = tf.keras.models.load_model(choosed_model)

"""Please note that for own models, both KNN similarity matching and cosine similarity based embeddings are implemented. This is contratory with Pretrained model where only cosine similarity based embeddings are implemented since it is only used for comparison purposes.

### KNN Similarity Matching

KNN similarity matching method didnt seem promising compared to the Cosine similarity matching method and hence it was only implemented for autoencoder_decoder_first_image_model

##### Extraction of features
"""

model = tf.keras.models.load_model("/content/drive/My Drive/Intelligence beyond fashion/autoencoder_decoder_first_image_model.h5")

"""
Extract features out of the model
@param - trained_model - trained model
@param - image_data - image data
@param - layer - the layer from which the features should be obtained
@return encoded array
"""
def extract_features(trained_model, image_data, layer=4):
    encoded = K.function([trained_model.layers[0].input], [trained_model.layers[layer].output])
    encoded_array = encoded([image_data])[0]
    pooled_array = encoded_array.max(axis=-1)
    return encoded_array

# Input the layer below according to the model 
# Here only for autoencoder_decoder_first_image_model and hence 9th layer should be correct
encoded = extract_features(model, train_data[:10], 9)

# Display the encoded detais for three different images
for index in [5,7,8]:
    plt.figure(figsize=(15,3))
    plot_custom(train_data[index],'','',1,4,1,"Original Image","","",'',True)
    plot_custom(encoded[index].mean(axis=-1),'','',1,4,2,"Encoded Mean","","",'',True)
    plot_custom(encoded[index].max(axis=-1),'','',1,4,3,"Encoded Std","","",'',True)
    plot_custom(encoded[index].std(axis=-1),'','',1,4,4,"Encoded Std","","",'',True)
    plt.show()

# Split the data into batches based on the batch size
def split_into_batches(data, batch_size=1000):
    if len(data) < batch_size:
        return [data]
    n_batches = len(data) // batch_size
    
    # If batches fit exactly into the size of data.
    if len(data) % batch_size == 0:
        return [data[i*batch_size:(i+1)*batch_size] for i in range(n_batches)]   

    # If there is a remainder.
    else:
        return [data[i*batch_size:min((i+1)*batch_size, len(data))] for i in range(n_batches+1)]

xf = []
counter = 0
concatenated_data = np.concatenate([train_data, test_data], axis=0)
for batch in split_into_batches(concatenated_data, batch_size=100):
    counter += 1
    encoded_data.append(extract_features(model, batch, 9))
encoded_data = np.concatenate(encoded_data)

X_encoded_reshaped_data = encoded_data.reshape(encoded_data.shape[0], encoded_data.shape[1] * encoded_data.shape[2] * encoded_data.shape[3])
np.save('/content/drive/My Drive/Intelligence beyond fashion/X_encoded_compressed.npy', X_encoded_reshaped_data)

X_encoded_reshaped_data = np.load('/content/drive/My Drive/Intelligence beyond fashion/X_encoded_compressed.npy')
X_encoded_reshaped_data.shape

# Perform dimensionality reduction using tsne
values = TSNE(n_components=2).fit_transform(X_encoded_reshaped_data)

lisp = input['image_path']

"""##### K-Means to cluster image data"""

num_clusters = [3, 4, 5, 6, 7]
for k in num_clusters:
    print("if Number of clusters: "+str(k))
    kmeans = KMeans(n_clusters=k, random_state=0).fit(X_encoded_reshaped_data)
    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_
    plt.figure(figsize=(10,5)) 
    plt.subplot(1,1,1)
    plt.scatter(values[:,0], values[:,1], c=kmeans.labels_.astype(float), s=50, alpha=0.5)
    plt.scatter(centroids[:, 0], centroids[:, 1], c=None, s=50)
    plt.show()
    for row in range(k): 
        iter=0
        plt.figure(figsize=(13,3))
        for i, iterator in enumerate(labels):
            print(i)
            if iterator == row:
                img = cv2.imread(lisp[i])
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                plot_(img,"","",1,6,iter+1,"cluster="+str(row),"","","",True)
                iter+=1
            if iter>=5: break
        plt.show()

# Training the kmeans. Also, finding the labels and centroids of the clusters
kmeans = KMeans(n_clusters = len(main_labels), random_state=0).fit(X_encoded_reshaped_data)
cluster_labels = kmeans.labels_
cluster_centers = kmeans.cluster_centers_
joblib.dump(kmeans, '/content/drive/My Drive/Intelligence beyond fashion/kmeans_model.pkl')

clusters_features_list = []
cluster_files_list = []
for i in range(len(main_labels)):
  i_cluster = []
  i_labels = []
  for j, label in enumerate(kmeans.labels_):
    if label == i:
      i_cluster.append(X_encoded_reshaped_data[j])
      i_labels.append(lisp[j])
  i_cluster = np.array(i_cluster)
  clusters_features_list.append(i_cluster)
  cluster_files_list.append(i_labels)

category_labels = []
features = []
filedata = []
for iter,i in enumerate(clusters_features_list):
  features.extend(i)
  category_labels.extend([iter for i in range(i.shape[0])])
  filedata.extend(cluster_files_list[iter])
print(np.array(category_labels).shape)
print(np.array(features).shape)
print(np.array(filedata).shape)

with open('/content/drive/My Drive/Intelligence beyond fashion/filedata', 'wb') as fp:
    pickle.dump(filedata, fp)

with open ('/content/drive/My Drive/Intelligence beyond fashion/filedata', 'rb') as fp:
    filedata = pickle.load(fp)

"""##### K-NN to find nearest neighbours"""

for i in [[3,5,7],[9,11,13]]:
    plt.figure(figsize=(25,5))
    for iter,j in enumerate(i):
        n_neighbors = j
        X = values  
        # y = labels
        y = [iter for i in range(int(X.size/2))]
        h = .09
        cmap_light = ListedColormap(['#FFB6C1', '#AAFFAA', '#AAAAFF','#E6E6FA','#8FBC8F','#DCDCDC'])
        cmap_bold = ListedColormap(['#F08080', '#00FF00', '#0000FF','#ADD8E6','#2F4F4F','#808080'])
        clf = KNeighborsClassifier(n_neighbors)
        clf.fit(X, y)

        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

        plt.subplot(1,3,iter+1)
        Z = Z.reshape(xx.shape)
        plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

        # Plot also the training points 
        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)
        plt.xlim(xx.min(), xx.max())
        plt.ylim(yy.min(), yy.max())
        plt.title("For n_neighbors = {}".format(j))
    plt.show()

knn = KNeighborsClassifier(n_neighbors=9, algorithm='ball_tree',n_jobs=-1)
knn.fit(np.array(features), np.array(cluster_labels))
joblib.dump(knn, '/content/drive/My Drive/Intelligence beyond fashion/knn_model.pkl')

"""##### Predictions finding"""

# Plot results
def display_results(query_index, result_indexes):
  def read_image(img_path):
    image = cv2.imread(img_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    return image

  plt.figure(figsize=(10, 5))
  if type(query_index) != type(30):
      plot_custom(query_index, "", "", 1, 1, 1, "Query Image", "", "", "", True)
  else:
      plot_custom(read_image(filedata[query_index]), "", "", 1, 1, 1, "Query Image " + filedata[query_index], "", "", "", True)
  plt.show()

  plt.figure(figsize=(20, 5))
  for iter, i in enumerate(result_indexes):
      plot_custom(read_image(filedata[i]), "", "", 1, len(result_indexes), iter+1, filedata[i], "", "", "", True)
  plt.show()

# Get the predicted images based on the given input
def show_predictions(category, label, N=8, is_url=False):
  image_array = []
  if is_url:
    img = io.imread(label)
    img = cv2.resize(img,(224,224))
    image_array.append(np.array(img))
  else:
    img_path = "./data/" + category + "/" + label
    img = load_img(img_path, target_size=(224,224))
    image_array.append(np.array(img))

  img_data = np.array(image_array)
  img_data = preprocess_input(img_data)

  feature = K.function([model.layers[0].input],[model.layers[9].output])
  output = feature([img_data])
  output = np.array(output).flatten().reshape(1,-1)
  res = knn.kneighbors(output.reshape(1,-1),return_distance=True,n_neighbors=N)
  display_results(img,list(res[1][0])[1:])

def predictions(category, label,N=8,isurl=False):
    image_array = []
    if isurl:
        img = io.imread(label)
        img = cv2.resize(img,(224,224))
        image_array.append(np.array(img))
    else:
        img_path = "./data/" + category + "/" + label
        img = load_img(img_path, target_size=(224,224))
        image_array.append(np.array(img))

    img_data = np.array(image_array)
    img_data = preprocess_input(img_data)

    feature = K.function([model.layers[0].input],[model.layers[9].output])
    output = feature([img_data])
    output = np.array(output).flatten().reshape(1,-1)
    res = knn.kneighbors(output.reshape(1,-1),return_distance=True,n_neighbors=N)
    display_results(img, list(res[1][0])[1:])

show_predictions("Backpacks", "1 image.jpg")

"""It is evident that image similarity using KNN method is not performing that good. Hence cosinge similarity matching is used.

### Cosine Similarity Matching
"""

# Generate the arc soft max layer for the images
class ArcLayer(tf.keras.layers.Layer):
  def __init__(self, num_classes, scale=30, margin=0.50, easy_margin=False,
              label_smoothing_eps=0.0, **kwargs):

      super(ArcLayer, self).__init__(**kwargs)

      self.num_classes = num_classes
      self.scale = scale
      self.margin = margin
      self.label_smoothing_eps = label_smoothing_eps
      self.easy_margin = easy_margin
      self.cos_m = tf.math.cos(margin)
      self.sin_m = tf.math.sin(margin)
      self.th = tf.math.cos(math.pi - margin)
      self.mm = tf.math.sin(math.pi - margin) * margin

  def get_config(self):
      config = super().get_config().copy()
      config.update({
          'num_classes': self.num_classes,
          'scale': self.scale,
          'margin': self.margin,
          'label_smoothing_eps': self.label_smoothing_eps,
          'easy_margin': self.easy_margin,
      })
      return config

  def build(self, input_shape):
      super(ArcLayer, self).build(input_shape[0])

      self.W = self.add_weight(
          name='W',
          shape=(int(input_shape[0][-1]), self.num_classes),
          initializer='glorot_uniform',
          dtype='float32',
          trainable=True,
          regularizer=None)

  def call(self, inputs):
      X, y = inputs
      y = tf.cast(y, dtype=tf.int32)
      cosine = tf.matmul(
          tf.math.l2_normalize(X, axis=1),
          tf.math.l2_normalize(self.W, axis=0)
      )
      sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))
      phi = cosine * self.cos_m - sine * self.sin_m
      if self.easy_margin:
          phi = tf.where(cosine > 0, phi, cosine)
      else:
          phi = tf.where(cosine > self.th, phi, cosine - self.mm)
      one_hot = tf.cast(
          tf.one_hot(y, depth=self.num_classes),
          dtype=cosine.dtype
      )
      if self.label_smoothing_eps > 0:
          one_hot = (1 - self.label_smoothing_eps) * one_hot + self.label_smoothing_eps / self.num_classes

      output = (one_hot * phi) + ((1.0 - one_hot) * cosine)
      output *= self.scale
      return output

# Generate arc margin for the pretained model using the arc layer
def generate_arc_margin(own_model):  
    arc_margin = ArcLayer(
            num_classes = 11014, 
            scale = 30, 
            margin = 0.5, 
            name='head/arc_margin', 
            dtype='float32'
            )
    input_img = tf.keras.layers.Input(shape = (224, 224, 3), name = 'image_input')
    input_label = tf.keras.layers.Input(shape = (), name = 'label_input')
    x = own_model(input_img)
    x = tf.keras.layers.GlobalAveragePooling2D(name="intermediate_output")(x)
    x = arc_margin([x, input_label])

    output = tf.keras.layers.Softmax(dtype='float32')(x)
    model = tf.keras.models.Model(inputs = [input_img, input_label], outputs = [output])
    return model

own_model = model
model = generate_arc_margin(own_model)
intermediate_model = tf.keras.models.Model(inputs=model.input,outputs=model.get_layer("intermediate_output").output)
image_model = intermediate_model
image_model.save('/content/drive/My Drive/Intelligence beyond fashion/reconstructive_own_image_model.h5')

def normalize_embeddings(embeddings):
    for embedding in embeddings:
        data = np.linalg.norm(embedding)
        embedding/=data
    return embeddings

# Get the image based on the image path
def generate_image_dataset(image_path, label):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img,channels=3)
    img = tf.image.resize(img,[224,224])
    return img, label

# JSON format with image data and label
def image_format(image,label):
    return {'image_input':image,'label_input':label},label

# Dataset function to obtain the image path and label from tensor slices
def get_dataset(image,label):
    dataset = tf.data.Dataset.from_tensor_slices((image,label))
    dataset = dataset.map(generate_image_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.map(image_format, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.batch(8)
    return dataset

# Generate arcface based embeddings of the model
chunk_size = 1024*1
num_chunks = len(input) // chunk_size
if len(input)%chunk_size != 0: num_chunks += 1
def generate_embeddings_arcface(model):
    image_embeddings = []
    for i, j in enumerate(range(num_chunks)):
        start_index = j * chunk_size
        end_index = min((j + 1) * chunk_size, len(input))
        print('Chunk:', start_index, 'to', end_index)
        curr_data = get_dataset(input.iloc[start_index:end_index].image_path.values, input.iloc[start_index:end_index].label_number.values)
        embeddings = model.predict(curr_data, verbose=1, use_multiprocessing=True, workers=-1)
        image_embeddings.append(embeddings)

    del model
    _ = gc.collect()
    
    return np.concatenate(image_embeddings)

own_model_embedding = generate_embeddings_arcface(intermediate_model)
own_model_embedding = normalize_embeddings(own_model_embedding)
np.save('/content/drive/My Drive/Intelligence beyond fashion/own_image_model_finetune_embedding.npy', own_model_embedding)
print(f"Shape of own model embeddings:{own_model_embedding.shape}")

own_model_embeddings = normalize_embeddings(np.load("/content/drive/My Drive/Intelligence beyond fashion/own_image_model_finetune_embedding.npy"))

def get_dataset(images_list):
    temp_label = pd.Series(-1).values
    filepaths = pd.Series(images_list).values
    dataset = tf.data.Dataset.from_tensor_slices((filepaths, temp_label))
    dataset = dataset.map(generate_image_dataset)
    dataset = dataset.map(image_format)
    dataset = dataset.batch(1)
    return dataset

image_path = "/content/drive/My Drive/Intelligence beyond fashion/test/input.jpg"

ds = get_dataset(image_path)

image_embedding = normalize_embeddings(image_model.predict(ds))

# Function to predict based on the cosine value
def cosine_value_predictions(embeddings, curr_embedding):
    cosine_value = np.matmul(embeddings, curr_embedding.T).T
    reshape_data = np.reshape(cosine_value > 0.0,(len(embeddings),))
    input['values'] = np.reshape(cosine_value,(len(embeddings),))
    return input[reshape_data].sort_values(by='values', ascending=False)

merged_image_predictions = cosine_value_predictions(own_model_embeddings, image_embedding)

top_15_image_predictions = merged_image_predictions.iloc[:15].reset_index()
top_15_image_predictions

"""## Pretrained Model

Please note that for pretrained model, only cosine similarity based embeddings are used.
"""

# Generate the arc soft max layer for the images
class ArcLayer(tf.keras.layers.Layer):
  def __init__(self, num_classes, scale=30, margin=0.50, easy_margin=False,
              label_smoothing_eps=0.0, **kwargs):

      super(ArcLayer, self).__init__(**kwargs)

      self.num_classes = num_classes
      self.scale = scale
      self.margin = margin
      self.label_smoothing_eps = label_smoothing_eps
      self.easy_margin = easy_margin
      self.cos_m = tf.math.cos(margin)
      self.sin_m = tf.math.sin(margin)
      self.th = tf.math.cos(math.pi - margin)
      self.mm = tf.math.sin(math.pi - margin) * margin

  def get_config(self):
      config = super().get_config().copy()
      config.update({
          'num_classes': self.num_classes,
          'scale': self.scale,
          'margin': self.margin,
          'label_smoothing_eps': self.label_smoothing_eps,
          'easy_margin': self.easy_margin,
      })
      return config

  def build(self, input_shape):
      super(ArcLayer, self).build(input_shape[0])

      self.W = self.add_weight(
          name='W',
          shape=(int(input_shape[0][-1]), self.num_classes),
          initializer='glorot_uniform',
          dtype='float32',
          trainable=True,
          regularizer=None)

  def call(self, inputs):
      X, y = inputs
      y = tf.cast(y, dtype=tf.int32)
      cosine = tf.matmul(
          tf.math.l2_normalize(X, axis=1),
          tf.math.l2_normalize(self.W, axis=0)
      )
      sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))
      phi = cosine * self.cos_m - sine * self.sin_m
      if self.easy_margin:
          phi = tf.where(cosine > 0, phi, cosine)
      else:
          phi = tf.where(cosine > self.th, phi, cosine - self.mm)
      one_hot = tf.cast(
          tf.one_hot(y, depth=self.num_classes),
          dtype=cosine.dtype
      )
      if self.label_smoothing_eps > 0:
          one_hot = (1 - self.label_smoothing_eps) * one_hot + self.label_smoothing_eps / self.num_classes

      output = (one_hot * phi) + ((1.0 - one_hot) * cosine)
      output *= self.scale
      return output

# Generate arc margin for the pretained model using the arc layer
def generate_arc_margin(pretrained_model):  
    arc_margin = ArcLayer(
            num_classes = 11014, 
            scale = 30, 
            margin = 0.5, 
            name='head/arc_margin', 
            dtype='float32'
            )
    input_img = tf.keras.layers.Input(shape = (256, 256, 3), name = 'image_input')
    input_label = tf.keras.layers.Input(shape = (), name = 'label_input')
    x = pretrained_model(input_img)
    x = tf.keras.layers.GlobalAveragePooling2D(name="intermediate_output")(x)
    x = arc_margin([x, input_label])

    output = tf.keras.layers.Softmax(dtype='float32')(x)
    model = tf.keras.models.Model(inputs = [input_img, input_label], outputs = [output])
    return model

pretrained_model = EfficientNetB5(weights=None, include_top=False, input_shape=None)
arc_margin_model = generate_arc_margin(pretrained_model)
arc_margin_model.load_weights('/content/drive/My Drive/Intelligence beyond fashion/pretrained_best_model_efnb5.h5')
pretrained_intermediate_model = tf.keras.models.Model(inputs=arc_margin_model.input, outputs=arc_margin_model.get_layer("intermediate_output").output)
pretrained_intermediate_model.save('/content/drive/My Drive/Intelligence beyond fashion/pretrained_reconstructive_image_model.h5')

image_model = pretrained_intermediate_model

def normalize_embeddings(embeddings):
    for embedding in embeddings:
        data = np.linalg.norm(embedding)
        embedding/=data
    return embeddings

# Get the image based on the image path
def generate_image_dataset(image_path, label):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img,channels=3)
    img = tf.image.resize(img,[256,256])
    return img, label

# JSON format with image data and label
def image_format(image,label):
    return {'image_input':image,'label_input':label},label

# Dataset function to obtain the image path and label from tensor slices
def get_dataset(image,label):
    dataset = tf.data.Dataset.from_tensor_slices((image,label))
    dataset = dataset.map(generate_image_dataset, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.map(image_format, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.batch(8)
    return dataset

# Generate arcface based embeddings of the model
chunk_size = 1024*1
num_chunks = len(input) // chunk_size
if len(input)%chunk_size != 0: num_chunks += 1
def generate_embeddings_arcface(model):
    image_embeddings = []
    for i, j in enumerate(range(num_chunks)):
        start_index = j * chunk_size
        end_index = min((j + 1) * chunk_size, len(input))
        print('Chunk:', start_index, 'to', end_index)
        curr_data = get_dataset(input.iloc[start_index:end_index].image_path.values, input.iloc[start_index:end_index].label_number.values)
        embeddings = model.predict(curr_data, verbose=1, use_multiprocessing=True, workers=-1)
        image_embeddings.append(embeddings)

    del model
    _ = gc.collect()
    
    return np.concatenate(image_embeddings)

pretrained_efficient_net_b5_embedding = generate_embeddings_arcface(pretrained_intermediate_model)
pretrained_efficient_net_b5_embedding = normalize_embeddings(pretrained_efficient_net_b5_embedding)
np.save('/content/drive/My Drive/Intelligence beyond fashion/pretrained_efficient_net_b5_finetune_embedding.npy', pretrained_efficient_net_b5_embedding)

pretrained_efnb5_embeddings = normalize_embeddings(np.load("/content/drive/My Drive/Intelligence beyond fashion/pretrained_efficient_net_b5_finetune_embedding.npy"))

image_path = "/content/drive/My Drive/Intelligence beyond fashion/test/input.jpg"

def get_dataset(images_list):
    temp_label = pd.Series(-1).values
    filepaths = pd.Series(images_list).values
    dataset = tf.data.Dataset.from_tensor_slices((filepaths, temp_label))
    dataset = dataset.map(generate_image_dataset)
    dataset = dataset.map(image_format)
    dataset = dataset.batch(1)
    return dataset

ds = get_dataset(image_path)

image_embedding = normalize_embeddings(image_model.predict(ds))

# Function to predict based on the cosine value
def cosine_value_predictions(embeddings, curr_embedding):
    cosine_value = np.matmul(embeddings, curr_embedding.T).T
    reshape_data = np.reshape(cosine_value > 0.0,(len(embeddings),))
    input['values'] = np.reshape(cosine_value,(len(embeddings),))
    return input[reshape_data].sort_values(by='values', ascending=False)

# Get merged image predictions
merged_image_predictions = cosine_value_predictions(pretrained_efnb5_embeddings, image_embedding)

# Finding top 15 image predictions
top_15_image_predictions = merged_image_predictions.iloc[:15].reset_index()
top_15_image_predictions

"""# **Combined Prediction**

In case of combined prediction, please note:


1.   For text model, import sentence transformer and sentence_transformer_text_embeddings for getting the predicted values
2.   For image model, import reconstructed_image_model and own_image_model_finetune_embedding for getting the predicted values

## Preparation
"""

source_data = input

# Text and Image model
text_model = SentenceTransformer('stsb-mpnet-base-v2') # Get the saved text model
reconstructed_image_model = tf.keras.models.load_model("/content/drive/My Drive/Intelligence beyond fashion/reconstructive_own_image_model.h5") # Get the saved image model

# Text embeddings
data = np.load("/content/drive/My Drive/Intelligence beyond fashion/sentence_transformer_text_embeddings.npy")
for value in data:
    normalized_value = np.linalg.norm(value)
    value/=normalized_value
sentence_transformer_embeddings = data

# Image embeddings
data = np.load("/content/drive/My Drive/Intelligence beyond fashion/own_image_model_finetune_embedding.npy")
for value in data:
    normalized_value = np.linalg.norm(value)
    value/=normalized_value
own_image_model_embeddings = data

# Combined embeddings
source_text_and_images = np.concatenate([own_image_model_embeddings, sentence_transformer_embeddings], axis=1)

# Image and Label in JSON format
def image_format(image, label):
    return {'image_input': image, 'label_input': label}, label

# Get image and label based on the image path and label
def data_manipulation(image_path,label):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img,[224,224])
    return img,label

"""## Prediction"""

text = "Women's light weight"
image_path = "/content/drive/My Drive/Intelligence beyond fashion/test/input.jpg"

# Encode the input text
data = text_model.encode(text)
for value in data:
    normalized_value = np.linalg.norm(value)
    value /= normalized_value
text_data = data

# Encode the input image
dataset = tf.data.Dataset.from_tensor_slices((pd.Series(image_path).values, pd.Series(-1).values)).map(data_manipulation).map(image_format).batch(1)
data = reconstructed_image_model.predict(dataset)
for value in data:
    normalized_value = np.linalg.norm(value)
    value /= normalized_value
image_data = value

# Combine the predictions and get the top specified predictions
N = 15
source_data['values'] = np.reshape(np.matmul(source_text_and_images, np.concatenate([image_data, text_data]).T).T, (len(source_text_and_images),))
top_n_predictions = source_data[np.reshape((np.matmul(source_text_and_images, np.concatenate([image_data, text_data]).T).T > 0.0), (len(source_text_and_images),))].sort_values(by='values', ascending=False).iloc[:N].reset_index()
top_n_predictions

"""# **References**

Chaitanyanarava. (2020, December 17). Image similarity model. Medium. https://medium.com/analytics-vidhya/image-similarity-model-6b89a22e2f1a

Keras. (n.d.). Image classification from scratch. Retrieved April 17, 2023, from https://keras.io/examples/vision/image_classification_from_scratch/

Manas Narkar. (2020, November 12). Image classification with convolution neural networks (CNN) with Keras. Pythian Blog. Retrieved April 17, 2023, from https://blog.pythian.com/image-classification-with-convolution-neural-networks-cnn-with-keras/

Micha Oleszak. (2022, March 11). Autoencoders: From vanilla to variational. Towards Data Science. Retrieved April 17, 2023, from https://towardsdatascience.com/autoencoders-from-vanilla-to-variational-6f5bb5537e4a

AI Stack Exchange. Retrieved April 17, 2023, from https://ai.stackexchange.com/questions/8885/why-is-the-variational-auto-encoders-output-blurred-while-gans-output-is-crisp

What is the difference between CNN and a convolutional autoencoder? Quora. Retrieved April 17, 2023, from https://www.quora.com/What-is-the-difference-between-CNN-and-a-convolutional-autoencoder

Building a recommendation system using CNN v2. Kaggle. Retrieved April 17, 2023, from https://www.kaggle.com/code/marlesson/building-a-recommendation-system-using-cnn-v2
"""